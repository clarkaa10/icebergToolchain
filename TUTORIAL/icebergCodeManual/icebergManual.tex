
\documentclass[12pt]{amsart}
\usepackage{graphicx}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage{hyperref}
\usepackage{listings}
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

% See the ``Article customise'' template for come common customisations

\title{Iceberg Mapping Toolchain User Guide}
\author{Marcus Hammond}
%\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle
\tableofcontents

This guide explains the usage of the iceberg mapping code toolchain I built in the course of my PhD. I apologize in advance, as it is not particularly streamlined.

By the end, the reader should be able to take raw mb88 files and produce a corrected point cloud of iceberg or canyon data. 

The instructions assume you're running Ubuntu. I haven't tested it on anything else. 

\section{Software Requirements}

Many of the requirements listed have a number of dependencies that must also be installed. I recommend using a package manager like apt-get or homebrew (or pip, for python) wherever possible. Some of them must be made from source.

\subsection{C++ compiler/cmake/make}
If you don't have a C++ compiler or the other standard tools like cmake and make, get them now. 

Go on, I'll wait...

I use gcc/g++.

\begin{lstlisting}
sudo apt-get install build-essential
\end{lstlisting}

\subsection{git}

This is a version control utility, like svn. Unlike svn, however, it has local repositories, so you can track changes and recover lost work/etc even without network connection. 

On Ubuntu: 
\begin{lstlisting}
sudo apt-get install git
\end{lstlisting}


\subsection{Python}
Great scripting language. It probably already came preinstalled, but I use the one from homebrew, since it plays nice with the other homebrew modules and pip for package installation.

\subsection{mbsystem}
This is a software library written and curated mostly by Dave Caress of MBARI. It interfaces with many types of sonar, and in the hands of a skilled user, is a very powerful tool for creating maps of the seafloor, etc. In the hands of unskilled users, it is a terrifying collection of functions and data structures with cryptic names and obscure purpose. Just trying to install it has been know to make grown men cry, but it is an ARL right of passage. Go boldly, and leave instructions with your next of kin should you not return.

Install instructions: \url{http://www.ldeo.columbia.edu/res/pi/MB-System/}

mbsystem cookbook (essentially, the manual): \url{http://www.mbari.org/data/mbsystem/mb-cookbook/}

\subsection{Point Cloud Library (pcl)}
This is an open source library for manipulating and visualizing point clouds of data. It's pretty slick, and is especially good when visualizing large datasets that would destroy matlab. It can be a little overwhelming to try to descend into the guts, especially if you're new to C++, but it has a lot of good tutorials and absolutely beats reinventing the wheel.

The one problem I had was that I could never get it to run on Mac OSX. As of June 2015 or so, there was an issue with VTK for OSX. VTK (The Visualization Toolkit) runs, you guessed it, the point cloud visualizations. By the time you read this, the issue may have been resolved. I haven't checked in a while.

Should be a pretty easy install if you go with the prebuilt binaries: 
\url{http://pointclouds.org/downloads/linux.html}

\subsection{OpenCV}
Computer vision! If you're only doing manual correspondence with the GUI, this really isn't necessary, but I'm a n00b at structuring C++ projects, so I think some modules think they depend on opencv when they don't really use it. Pure laziness/ignorance. Sorry.

\url{http://docs.opencv.org/doc/tutorials/introduction/linux_install/linux_install.html}


\subsection*{QT}

This is a framework for constructing GUIs. 

\url{http://www.qt.io/developers/}

\subsection*{Ceres}

This is a Google nonlinear least squares solver and does the heavy lifting for the GraphSLAM optimization.

\url{http://ceres-solver.org/}

\subsection{icebergToolchain}

You'll [obviously] need the iceberg code too. For now, it's on github:

Navigate to where you want the toolchain to be located and type

\begin{lstlisting}
git clone https://github.com/thehammstr/icebergToolchain.git
\end{lstlisting}

\section{Code Structure}

All code is contained in the git project ``icebergToolchain," and arranged into subfolders roughly by function.

\subsection{dataExtractionAndProcessing}

You guessed it: this folder houses code to extract relevant information from mbsystem and process it into a format that plays nice with the other code modules. No motion estimation or loop closure detection is done here.

\subsection{Generic}

Class definitions and functions that are useful across multiple modules.

\subsection{manualCorrespondence}

This folder houses the GUI for interactively detecting and encoding loop closure information 

\subsection{featureExtraction}

All pseudo-image processing code, including the automated loop closure detector. Upon writing this, there still isn't great integration between this and the manual GUI module.

\subsection{ceres}

This is where the GraphSLAM is done. The name comes from the Google Ceres solver.



\section{Procedure}

The process to take mb88 files and turn them into an octree-ready point cloud has several steps. It would be amazing for it to all be integrated into the GUI one day, but as of now it is not. It is also quite inefficient with data storage: most of the intermediate files are human-readable .csv files, which take up much more space than binaries.

\subsection{Select region of interest using mbsystem}

Narrow down the area of data you care about by using the mbdatalist command line tool. 
\vspace{12pt}
\begin{tiny}
\begin{lstlisting}
#!/bin/bash 
# extract mb88 files
ls -1 | grep mb88$ > tmplist
# make datalist
mbdatalist -F-1 -I tmplist -R-121.995833/-121.9875/36.776389/36.780556 \ > wall-datalist2
mbm_plot -F-1 -I wall-datalist2 -N -C -G1 
./wall-datalist.cmd 
\end{lstlisting}
\end{tiny}
\vspace{12pt}
Now a line-by-line breakdown:
\begin{lstlisting}
ls -1 | grep mb88$ > tmplist
\end{lstlisting}
This just dumps all mb88 filenames into a list that mbsystem can use
\vspace{12pt}
\begin{tiny}
\begin{lstlisting}
mbdatalist -F-1 -I tmplist -R-121.995833/-121.9875/36.776389/36.780556 \ > wall-datalist
\end{lstlisting}
\end{tiny}
\normalsize
There's a lot going on here. The important part is that you're calling mbdatalist on the input file `tmplist', which contains all the mb88 files, then using the -R command to limit the data to a box defined by the four numbers to follow. They are lon-min/lonmax/latmin/latmax. You then store the output into another datalist, in this case, `wall-datalist.'

The last two lines of code just use the built-in mbsystem tools to display the data you've just extracted, to make sure it's what you want. the wall-datalist file (or whatever you want to name it) is the input for the next step in the process.

The file ``dataExtractionAndProcessing/getThatData.sh" can be run within a file containing a bunch of mb88 files and should do this. You'll need to go in and change the lat-lon boundaries if you're processing something different from Soquel canyon.


\subsection{Convert mb88 files to csv}

MBsystem scares me, so I touched the code as little as possible to get the data I needed. Think of it as a special forces hostage rescue situation, except instead of Navy Seals, you have The Three Stooges. Most of the code I stripped from examples written by Dave Caress for the mbsystem developers workshop. I was just trying not to break something. 

The file of interest is 
\begin{lstlisting}
dataExtractionAndProcessing/extractWallData.c
\end{lstlisting}
which can be compiled using \textbf{make}.
\vspace{12pt}
Then call it using the following format
\begin{small}
\begin{lstlisting}
./extractWallData -F-1 -I [path to datalist] -O [path to output csv]
\end{lstlisting}
\end{small}
The -F is a formatting flag. The -I is a flag denoting input. The [path to datalist] refers to the datalist you created with the mbdatalist command in the previous section. If the -O option is omitted, it will default to ``extractedData.csv" in the current folder.

This dumps everything into an enormous csv file, with one beam per line. There's a lot of redundant information, since each beam has its corresponding pose listed as well. It probably wouldn't take too much work to clean this up in the C code, but I ended up doing it in python in the next section.  

\subsection{Clean data}
The next call is 
\begin{lstlisting}
dataExtractionAndProcessing/cleanData.py [input file] [output folder]
\end{lstlisting}

In this step, the data is reformatted so that each line in the data file only has \emph{one pose} with many sonar readings. It also applies a few heuristics to scrub the data, taking out spurious soundings. It clusters soundings, and any clusters with fewer than a given number of members are deleted. This reduces ``speckle" noise, but isn't perfect. This file also performs subsampling. Setting the variable \_SUBSAMPLE\_ at the top of the file to `N' extracts every Nth range from the scan (i.e. \_SUBSAMPLE\_ = 3 reduces the density of the point cloud by a factor of 3).

The script places a file called ``state\_and\_ranges.csv" into the output folder. This is transformed so that the first record is at the origin. This is the main data structure used as input/output for the loop closure and optimization. 

The script also creates a file called ``fullstatehist.csv" in the same folder, which includes the original nav data.

Each row of the output has the columns:\\
time, x, y, z, phi, theta, psi, commanded speed, $\omega_z$, dvl flag, DVL, multibeam measurements

The dvl flag states whether the DVL was in lock. The dvl measurements are the body-xyz velocity readings. Each multibeam measurement is four elements long. The first three are the vehicle body-xyz components of the beam. The fourth was included to capture curvature information, but is not currently used.

\subsection{Automated Correspondence Detection}


This functionality was broken in the conversion to OpenCV3.0. SIFT is no longer present in the standard library distribution, and I haven't yet figured out how to put it back. 

The files of interest are
\begin{lstlisting}
featureExtraction/build/pullImages
featureExtraction/python/analyzeRangeImages
\end{lstlisting}

The former converts sonar point cloud submaps to range images, while the latter looks (looked?) for matches between these range images. But as I said, it's been broken since converting to the new version of opencv.

\subsection{Manual correspondence GUI}

This thing is fun/useful. The source code is in 
\begin{lstlisting}
manualCorrespondence/src
\end{lstlisting}

To build, navigate to manualCorrespondence/build, then type 

\begin{lstlisting}
cmake ../src
make
\end{lstlisting}

If there are no errors, you'll see the executable 
\begin{lstlisting}
manualCorrespondence/build/correspondenceGUI
\end{lstlisting}

Call this with the paths to the input state\_and\_ranges.csv file, as well as a recordedLinks.csv file, if it exists.

\begin{lstlisting}
./correspondenceGUI [path to input] [link file name (optional)]
\end{lstlisting}

Once this is open, you can find correspondences to your heart's content. The GUI user guide is in the appendix of my dissertation.

\subsection{GraphSLAM with ceres}

The two important files in this step are 
\begin{lstlisting}
ceres/solveIcebergPosesOnly.cc
ceres/ResidualBlockDefinitions.h
\end{lstlisting}

The solveIcebergPosesOnly.cc reads in the data and link file, and builds the ceres problem. 

ResidualBlockDefinitions.h defines the components of the cost function, i.e. defines the cost function and weight associated with each type of link in the GraphSLAM problem. This is where the bulk of solution knob-twiddling occurs. For example:
\begin{lstlisting}
	if (pose1.DVLflag){ // check for dvl lock
	residual[_X_] = 1.*Xres ;
	residual[_Y_] = 1.*Yres ;
	} else {
	residual[_X_] = .8*Xres ;
	residual[_Y_] = .8*Yres ;
	}
\end{lstlisting}

The ``magic numbers" just to the right of the equals sign are the weights. These should all be \#defined somewhere, but I haven't gotten around to it. 

Compile the solver with the commands 
\begin{lstlisting}
cmake .
make
\end{lstlisting}

Then run the program 
\begin{lstlisting}
./solveIceberg --input=[input data file] --links=[link file]
\end{lstlisting}

This will take a while, and will put a modified `state\_and\_ranges' file in the same folder as the input. This can be reopened with the manualCorrespondence GUI if necessary to refine the matches. It also produces a file called `output\_cloud.pcd' and another called `output\_cloud.pcd' in the current directory. The former is the corrected point cloud, and the latter is the corrected trajectory, in XYZRGB format.

\subsection{Manual correspondence GUI}

The manual correspondence GUI reads the ``state\_and\_ranges.csv" file created by ``cleandata.py" and allows the user to detect and encode loop closure events. The resulting loop closure info is saved to a file with default name ``recordedLinks.csv." 

Each line of the file has the format:
pose1 index, pose2 index, [16 Transform elements]

The transform is expressed as the rows of a 4x4 homogeneous matrix that maps a point expressed in the vehicle frame at the second pose index to the vehicle frame at the first index.

\subsubsection{Compiling and launching the GUI}

Navigate to the folder named ``manualCorrespondence." If it doesn't exist already, create a ``build folder."

\begin{lstlisting}
cd manualCorrespondence/build
cmake ../src
make
\end{lstlisting} 

If no errors were encountered, you should have an executable called ``correspondenceGUI" in the build folder. Run this with the path to your state and range csv file as the first argument and the linkfile as an optional second argument.

\subsubsection{Using the GUI}

\include{GUImanual}

\subsection{Solving with Ceres}
Now to do GraphSLAM
\subsubsection{Building}

The two most important files are solverIcebergPosesOnly.cc and ResidualBlockDefinitions.h, already discussed above. To build the ceres-powered executable, do the following:

\begin{lstlisting}
cd ceres
cmake .
make
\end{lstlisting} 



\subsubsection{Running} Enter the following command:
\begin{tiny}
\begin{lstlisting}
./solveIceberg --input=[state\_and\_ranges.csv file location] --links=[recordedLinks.csv file location]
\end{lstlisting} 
\end{tiny}
This will start churning. Some displays will pop up for debugging, and eventually ceres will run. The output will be saved in several forms/formats:

First, a new state\_and\_ranges\_modified file will be created in the same folder as the original. This can be read by the GUI in order to iterate and encode more loop closures. The multibeam point cloud will be saved as a pcd file (pcl's standard format) in the ceres folder as ``output.pcd'' and the final trajectory point cloud will also be saved as ``output\_traj.pcd.''


The file \emph{buildandrun.sh} combines all these steps for convenience.

The pcd files can be converted to csv files of just xyz data using 
\begin{lstlisting}
Generic/pcd2xyz.py .
\end{lstlisting}
\end{document}

